{
 "nbformat": 4,
 "nbformat_minor": 0,
 "metadata": {
  "colab": {
   "provenance": [],
   "gpuType": "T4",
   "collapsed_sections": [
    "9FoY3v4uShpD"
   ]
  },
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3"
  },
  "language_info": {
   "name": "python"
  },
  "accelerator": "GPU"
 },
 "cells": [
  {
   "cell_type": "markdown",
   "source": [
    "# Install"
   ],
   "metadata": {
    "id": "9FoY3v4uShpD"
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "To install run these commands"
   ],
   "metadata": {
    "id": "UFYusIl1SUh8"
   }
  },
  {
   "cell_type": "code",
   "source": [
    "from google.colab import drive\n",
    "drive.mount('/content/drive')\n",
    "%cd /content/drive/My\\ Drive/\n",
    "!mkdir ludmila\n",
    "%cd ludmila/"
   ],
   "metadata": {
    "id": "FrZP9EeuSwuQ"
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "%cd /content/drive/My\\ Drive/ludmila\n",
    "!git clone https://github.com/nevstas/ludmila.git\n",
    "%cd /content/drive/My\\ Drive/ludmila/ludmila"
   ],
   "metadata": {
    "id": "lWm1hcCiS3WY"
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Run after connect/reconnect"
   ],
   "metadata": {
    "id": "5P3EUC2kTJTQ"
   }
  },
  {
   "cell_type": "code",
   "source": [
    "from google.colab import drive\n",
    "drive.mount('/content/drive', force_remount=False)\n",
    "%cd /content/drive/My\\ Drive/ludmila/ludmila"
   ],
   "metadata": {
    "id": "lM97XPuDTQsj",
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "outputId": "93b9978c-8641-4f1d-9bcf-5eb6e50f0c9a"
   },
   "execution_count": 31,
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n",
      "/content/drive/My Drive/ludmila/ludmila\n"
     ]
    }
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Run"
   ],
   "metadata": {
    "id": "r-rtVPFgTU5O"
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "Ludmila GPU"
   ],
   "metadata": {
    "id": "mKSearE72Nnk"
   }
  },
  {
   "cell_type": "code",
   "source": [
    "import time\n",
    "import itertools\n",
    "import torch\n",
    "import threading\n",
    "from threading import Lock\n",
    "\n",
    "myLock = threading.Lock()\n",
    "\n",
    "script_path = \"/content/drive/My Drive/ludmila/ludmila\"\n",
    "\n",
    "dataset_id = 1\n",
    "dataset_filename = \"data\" + str(dataset_id) + \".txt\"\n",
    "\n",
    "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "print(\"Device:\", device)\n",
    "\n",
    "# X range\n",
    "start, end = -10, 10\n",
    "dtype = torch.int64\n",
    "\n",
    "def fmt_time(t):\n",
    "    ms = int((t % 1) * 1000)\n",
    "    sec = int(t) % 60\n",
    "    mins = int(t // 60)\n",
    "    return f\"{mins} min {sec} sec {ms} ms\"\n",
    "\n",
    "def op_add(a, b):\n",
    "    return a + b, torch.ones_like(a, dtype=torch.bool)\n",
    "\n",
    "def op_sub(a, b):\n",
    "    return a - b, torch.ones_like(a, dtype=torch.bool)\n",
    "\n",
    "def op_mul(a, b):\n",
    "    return a * b, torch.ones_like(a, dtype=torch.bool)\n",
    "\n",
    "def op_div(a, b):\n",
    "    # strict integer divisibility and ban on division by 0\n",
    "    valid = (b != 0) & (a.remainder(b) == 0)\n",
    "    out = torch.empty_like(a)\n",
    "    if valid.any():\n",
    "        out[valid] = a[valid] // b[valid]\n",
    "    if (~valid).any():\n",
    "        out[~valid] = 0\n",
    "    return out, valid\n",
    "\n",
    "def op_pow2(a, _b_ignored):\n",
    "    # (a)^2, always valid\n",
    "    return a * a, torch.ones_like(a, dtype=torch.bool)\n",
    "\n",
    "def op_sqrt(a, _b_ignored):\n",
    "    # sqrt(a) only for a >= 0 and only if a is a perfect square\n",
    "    valid = (a >= 0)\n",
    "\n",
    "    # get integer root\n",
    "    r = torch.sqrt(a.float()).to(torch.int64)\n",
    "\n",
    "    # check accuracy: r*r == a\n",
    "    valid = valid & ((r * r) == a)\n",
    "\n",
    "    out = torch.empty_like(a)\n",
    "    if valid.any():\n",
    "        out[valid] = r[valid]\n",
    "    if (~valid).any():\n",
    "        out[~valid] = 0\n",
    "    return out, valid\n",
    "\n",
    "\n",
    "OPS = {\n",
    "    '+': op_add,\n",
    "    '-': op_sub,\n",
    "    '*': op_mul,\n",
    "    '/': op_div,\n",
    "    '^2': op_pow2,\n",
    "    '^0.5': op_sqrt,\n",
    "}\n",
    "\n",
    "OPS_FIRST_PASS = {'*', '/', '^2'}\n",
    "\n",
    "def operand_tensor(token, x_vals):\n",
    "    if token == 'x':\n",
    "        return x_vals\n",
    "    else:\n",
    "        return torch.full_like(x_vals, int(token), dtype=x_vals.dtype, device=x_vals.device)\n",
    "\n",
    "def eval_expr_tokens(tokens, ops, x_vals):\n",
    "    has_x = any(tok == 'x' for tok in tokens)\n",
    "    if has_x:\n",
    "        base = x_vals\n",
    "        valid = torch.ones_like(x_vals, dtype=torch.bool)\n",
    "    else:\n",
    "        base = torch.tensor([0], device=x_vals.device, dtype=x_vals.dtype)\n",
    "        valid = torch.ones(1, dtype=torch.bool, device=x_vals.device)\n",
    "\n",
    "    vals = [operand_tensor(tok, base) for tok in tokens]\n",
    "\n",
    "    # --- First pass: *, /, ^2 ---\n",
    "    vals2 = [vals[0]]\n",
    "    ops2 = []\n",
    "    trailing_sqrt = False\n",
    "    n_ops = len(ops)\n",
    "\n",
    "    for i, sym in enumerate(ops):\n",
    "        b = vals[i + 1]\n",
    "        is_last = (i == n_ops - 1)\n",
    "        if sym in OPS_FIRST_PASS:\n",
    "            fn = OPS[sym]\n",
    "            cur, v_step = fn(vals2[-1], b)\n",
    "            vals2[-1] = cur\n",
    "            valid = valid & v_step\n",
    "            if has_x and not valid.any(): break\n",
    "            if not has_x and not bool(valid.item()): break\n",
    "        elif sym == '^0.5' and is_last:\n",
    "            # postpone sqrt until the very end (after + and -)\n",
    "            trailing_sqrt = True\n",
    "            # IMPORTANT: do not add b, it is \"dummy\" for unary operation\n",
    "        else:\n",
    "            # + or - (and any other operator if you want to extend)\n",
    "            ops2.append(sym)\n",
    "            vals2.append(b)\n",
    "\n",
    "    # Early exit\n",
    "    if has_x and not valid.any():\n",
    "        return torch.zeros_like(x_vals), torch.zeros_like(x_vals, dtype=torch.bool), has_x\n",
    "    if not has_x and not bool(valid.item()):\n",
    "        out = torch.zeros_like(x_vals)\n",
    "        msk = torch.zeros_like(x_vals, dtype=torch.bool)\n",
    "        return out, msk, has_x\n",
    "\n",
    "    # --- Second pass: + and - ---\n",
    "    res = vals2[0]\n",
    "    for i, sym in enumerate(ops2):\n",
    "        fn = OPS[sym]\n",
    "        b = vals2[i + 1]\n",
    "        res, v_step = fn(res, b)\n",
    "        valid = valid & v_step\n",
    "        if has_x and not valid.any(): break\n",
    "        if not has_x and not bool(valid.item()): break\n",
    "\n",
    "    # --- Deferred sqrt at the very end ---\n",
    "    if trailing_sqrt:\n",
    "        res, v_step = OPS['^0.5'](res, res)   # b is ignored\n",
    "        valid = valid & v_step\n",
    "\n",
    "    if not has_x:\n",
    "        res = res.expand_as(x_vals)\n",
    "        valid = valid.expand_as(x_vals)\n",
    "\n",
    "    return res, valid, has_x\n",
    "\n",
    "def build_formula(tokens, x_value=None):\n",
    "    parts = []\n",
    "    for i, tok in enumerate(tokens):\n",
    "        val = str(x_value) if (tok == 'x' and x_value is not None) else str(tok)\n",
    "        parts.append(val)\n",
    "        if i < len(tokens) - 1:\n",
    "            parts.append(None)  # placeholder for operator\n",
    "    return parts\n",
    "\n",
    "def stringify(parts, ops):\n",
    "    out = []\n",
    "    op_iter = iter(ops)\n",
    "    for p in parts:\n",
    "        if p is None:\n",
    "            try:\n",
    "                out.append(next(op_iter))\n",
    "            except StopIteration:\n",
    "                # Just in case, but normally won't get here\n",
    "                pass\n",
    "        else:\n",
    "            out.append(str(p))\n",
    "    # If there are leftover operators (e.g., final unary ^0.5) — append them at the end\n",
    "    for sym in op_iter:\n",
    "        out.append(sym)\n",
    "    return \" \".join(out)\n",
    "\n",
    "\n",
    "def writeln(str):\n",
    "    myLock.acquire()\n",
    "    with open(script_path + \"/log.txt\", 'a', encoding='utf-8') as the_file:\n",
    "        the_file.write(str + \"\\n\")\n",
    "    myLock.release()\n",
    "\n",
    "def load_dataset(path):\n",
    "    \"\"\"\n",
    "    Expects lines of the format:\n",
    "    y [c1 c2 c3 ...]\n",
    "    Returns a list: [(y, [c1, c2, c3, ...]), ...]\n",
    "    The number of constants after y can be arbitrary (including 0).\n",
    "    Empty lines are ignored.\n",
    "    \"\"\"\n",
    "    ds = []\n",
    "    with open(path, 'r', encoding='utf-8') as f:\n",
    "        for line in f:\n",
    "            line = line.strip()\n",
    "            if not line:\n",
    "                continue\n",
    "            parts = line.split()\n",
    "            # at minimum — one number (y)\n",
    "            try:\n",
    "                yv = int(parts[0])\n",
    "            except ValueError:\n",
    "                # line is invalid — skip it\n",
    "                continue\n",
    "            consts = []\n",
    "            for p in parts[1:]:\n",
    "                try:\n",
    "                    consts.append(int(p))\n",
    "                except ValueError:\n",
    "                    # invalid token — skip only this one\n",
    "                    continue\n",
    "            ds.append((yv, consts))\n",
    "    return ds\n",
    "\n",
    "\n",
    "# --- Mapping constants by indices (key to universal check) ---\n",
    "def make_const_index_map(const_pool):\n",
    "    \"\"\"\n",
    "    Returns dict: {\"51\": 0, \"62\": 1, \"73\": 2} for the current pool.\n",
    "    \"\"\"\n",
    "    return {str(v): i for i, v in enumerate(const_pool)}\n",
    "\n",
    "def remap_tokens_to_target_consts(tokens, base_const_pool, target_consts):\n",
    "    \"\"\"\n",
    "    Transfers the structure of tokens, replacing constants from the base pool\n",
    "    with the corresponding constants of target_consts by INDEX.\n",
    "    Example: tokens = ['x','*','51','+','73'] with target=[52,63,74]\n",
    "    -> ['x','*','52','+','74']\n",
    "    \"\"\"\n",
    "    idx_map = make_const_index_map(base_const_pool)\n",
    "    out = []\n",
    "    for tok in tokens:\n",
    "        if tok == 'x':\n",
    "            out.append('x')\n",
    "        else:\n",
    "            # this is a constant from the base pool\n",
    "            k = idx_map.get(tok, None)\n",
    "            if k is None:\n",
    "                # Protection: if tokens unexpectedly contain a number not from the pool\n",
    "                out.append(tok)\n",
    "            else:\n",
    "                out.append(str(target_consts[k]))\n",
    "    return out\n",
    "\n",
    "def validate_formula_on_all_sets(tokens_base, ops):\n",
    "    \"\"\"\n",
    "    Checks the formula (structure tokens_base + ops), selected on the first set,\n",
    "    on all sets of the dataset. For each set substitutes its constants\n",
    "    by indices and checks if there exists at least one x in the range\n",
    "    such that res == y of the set with valid operations.\n",
    "    Returns (ok, xs_per_set) — ok: bool; xs_per_set: list of found x or None.\n",
    "    \"\"\"\n",
    "    xs_demo = []  # for info: which x were found on the sets\n",
    "    for (y_target, consts_target) in dataset:\n",
    "        # Transfer tokens to constants of this set\n",
    "        tokens_target = remap_tokens_to_target_consts(tokens_base, CONST_POOL_BASE, consts_target)\n",
    "        # Compute\n",
    "        res, valid, has_x = eval_expr_tokens(tokens_target, ops, x_vals)\n",
    "        hits = torch.nonzero(valid & (res == y_target), as_tuple=False).flatten()\n",
    "        if hits.numel() == 0:\n",
    "            return False, None\n",
    "        # Save one demo x\n",
    "        xs_demo.append(int(x_vals[hits[0]].item()) if has_x else None)\n",
    "    return True, xs_demo\n",
    "\n",
    "if device == 'cuda':\n",
    "    torch.cuda.synchronize()\n",
    "t0 = time.perf_counter()\n",
    "\n",
    "# Load the whole dataset\n",
    "dataset_path = script_path + \"/datasets/\" + dataset_filename\n",
    "dataset = load_dataset(dataset_path)\n",
    "if not dataset:\n",
    "    raise RuntimeError(f\"Dataset is empty or not read: {dataset_path}\")\n",
    "\n",
    "# The first set is the base for initial search\n",
    "# CHANGED: no checks or comparisons — just take the first line from the file\n",
    "y_base, CONST_POOL_BASE = dataset[0]  # CHANGED\n",
    "\n",
    "x_vals = torch.arange(start, end + 1, dtype=dtype, device=device)\n",
    "\n",
    "OPERAND_POOL_BASE = ['x'] + [str(c) for c in CONST_POOL_BASE]\n",
    "\n",
    "# stats:\n",
    "checked_exprs = 0\n",
    "solutions_found_global = 0  # how many \"universal\" formulas found and logged\n",
    "attempted_eqs_total_base = 0                  # (2a) scalar checks f(x)=y on the base set\n",
    "attempted_eqs_by_len_base = {L: 0 for L in range(1, 6)}\n",
    "\n",
    "time_total_start = time.time()\n",
    "\n",
    "for length in range(1, 6):  # operand length\n",
    "    # all operand sequences\n",
    "    for tokens in itertools.product(OPERAND_POOL_BASE, repeat=length):\n",
    "        # all operator sequences of corresponding length\n",
    "        OPS_ALPHABET = ['+', '-', '*', '/', '^2', '^0.5']\n",
    "\n",
    "        if length == 1:\n",
    "            ops_list = [()]\n",
    "        else:\n",
    "            ops_list = itertools.product(OPS_ALPHABET, repeat=length - 1)\n",
    "\n",
    "        for ops in ops_list:\n",
    "            # Compute on the BASE set (as before)\n",
    "            res, valid, has_x = eval_expr_tokens(tokens, ops, x_vals)\n",
    "            hits = torch.nonzero(valid & (res == y_base), as_tuple=False).flatten()\n",
    "\n",
    "            n_valid_base = int(valid.sum().item())\n",
    "\n",
    "            # stats:\n",
    "            checked_exprs += 1\n",
    "            attempted_eqs_total_base += n_valid_base\n",
    "            attempted_eqs_by_len_base[length] += n_valid_base\n",
    "\n",
    "            if hits.numel() == 0:\n",
    "                continue\n",
    "\n",
    "            # Previously we logged right away. Now — first validate on ALL sets.\n",
    "            ok, xs_demo = validate_formula_on_all_sets(list(tokens), ops)\n",
    "            if not ok:\n",
    "                # Formula is not universal — skip without logs\n",
    "                continue\n",
    "\n",
    "            # Universal formula found — log ONCE\n",
    "            # Show the formula with x substituted from the first hit on the base set\n",
    "            x_found_base = int(x_vals[hits[0]].item()) if has_x else None\n",
    "            parts = build_formula(tokens, x_value=x_found_base)\n",
    "            formula_str = stringify(parts, ops)\n",
    "            time_total = time.time() - time_total_start\n",
    "            message = time.strftime(\"%d.%m.%Y %H:%M:%S\") + \" Solution data\" + str(dataset_id) + \": \" + formula_str + \" at \" + str(round(time_total, 2)) + \" seconds\"\n",
    "\n",
    "            print(message)\n",
    "            writeln(message)\n",
    "            solutions_found_global += 1\n",
    "            # We may NOT stop search — let it find alternative universal formulas\n",
    "            # If you want to stop at the first one — uncomment the next line:\n",
    "            # raise SystemExit\n",
    "\n",
    "if device == 'cuda':\n",
    "    torch.cuda.synchronize()\n",
    "t1 = time.perf_counter()\n",
    "\n",
    "# ---------------- OUTPUT ----------------\n",
    "elapsed = t1 - t0\n",
    "\n",
    "if solutions_found_global == 0:\n",
    "    print(\"No universal solutions found in the given range and set of expressions.\")\n",
    "else:\n",
    "    print(f\"Total universal formulas: {solutions_found_global}\")\n",
    "\n",
    "print(f\"Base constant pool: {CONST_POOL_BASE}\")\n",
    "print(f\"X range: [{start}, {end}]\")\n",
    "print(f\"Checked expressions (combinations) on the base set: ~{checked_exprs:,}\")\n",
    "print(f\"Time: {fmt_time(elapsed)}\")\n"
   ],
   "metadata": {
    "id": "mKivqcjF2VDb",
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 297
    },
    "outputId": "6ee711c8-5624-4686-b74a-7a4d025e94b1"
   },
   "execution_count": 42,
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Device: cuda\n",
      "06.09.2025 18:52:22 Solution data1: 51 * 62 + 73 at 0.55 seconds\n",
      "06.09.2025 18:52:23 Solution data1: 62 * 51 + 73 at 0.79 seconds\n",
      "06.09.2025 18:52:23 Solution data1: 73 + 51 * 62 at 1.08 seconds\n",
      "06.09.2025 18:52:23 Solution data1: 73 + 62 * 51 at 1.16 seconds\n"
     ]
    },
    {
     "output_type": "error",
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "traceback": [
      "\u001B[0;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[0;31mKeyboardInterrupt\u001B[0m                         Traceback (most recent call last)",
      "\u001B[0;32m/tmp/ipython-input-2158008740.py\u001B[0m in \u001B[0;36m<cell line: 0>\u001B[0;34m()\u001B[0m\n\u001B[1;32m    303\u001B[0m             \u001B[0;31m# Compute on the BASE set (as before)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m    304\u001B[0m             \u001B[0mres\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mvalid\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mhas_x\u001B[0m \u001B[0;34m=\u001B[0m \u001B[0meval_expr_tokens\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mtokens\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mops\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mx_vals\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0;32m--> 305\u001B[0;31m             \u001B[0mhits\u001B[0m \u001B[0;34m=\u001B[0m \u001B[0mtorch\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mnonzero\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mvalid\u001B[0m \u001B[0;34m&\u001B[0m \u001B[0;34m(\u001B[0m\u001B[0mres\u001B[0m \u001B[0;34m==\u001B[0m \u001B[0my_base\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mas_tuple\u001B[0m\u001B[0;34m=\u001B[0m\u001B[0;32mFalse\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mflatten\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0m\u001B[1;32m    306\u001B[0m \u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m    307\u001B[0m             \u001B[0mn_valid_base\u001B[0m \u001B[0;34m=\u001B[0m \u001B[0mint\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mvalid\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0msum\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mitem\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n",
      "\u001B[0;31mKeyboardInterrupt\u001B[0m: "
     ]
    }
   ]
  }
 ]
}
