{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# Install on Google Colab"
      ],
      "metadata": {
        "id": "9FoY3v4uShpD"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')\n",
        "%cd /content/drive/My\\ Drive/\n",
        "!mkdir ludmila\n",
        "%cd ludmila/"
      ],
      "metadata": {
        "id": "FrZP9EeuSwuQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "%cd /content/drive/My\\ Drive/ludmila\n",
        "!git clone https://github.com/nevstas/ludmila.git\n",
        "%cd /content/drive/My\\ Drive/ludmila/ludmila"
      ],
      "metadata": {
        "id": "lWm1hcCiS3WY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Install on RunPod"
      ],
      "metadata": {
        "id": "X9940sjK9UC2"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "%cd /root/\n",
        "!mkdir ludmila\n",
        "%cd ludmila/"
      ],
      "metadata": {
        "id": "c4T9wQTt9eMI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "%cd /root/ludmila\n",
        "!git clone https://github.com/nevstas/ludmila.git\n",
        "%cd /root/ludmila/ludmila"
      ],
      "metadata": {
        "id": "1wYuXCk39jzm"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Run after connect/reconnect Google Colab"
      ],
      "metadata": {
        "id": "5P3EUC2kTJTQ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive', force_remount=False)\n",
        "%cd /content/drive/My\\ Drive/ludmila/ludmila"
      ],
      "metadata": {
        "id": "lM97XPuDTQsj",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "05cac0ed-ea67-49ed-b504-5c5823878b2c"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n",
            "/content/drive/My Drive/ludmila/ludmila\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Run after connect/reconnect RunPod"
      ],
      "metadata": {
        "id": "tUaInBlv9st1"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "%cd /root/ludmila/ludmila"
      ],
      "metadata": {
        "id": "KHfPYcMb9yTY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Run"
      ],
      "metadata": {
        "id": "r-rtVPFgTU5O"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import time\n",
        "import itertools\n",
        "import torch\n",
        "import threading\n",
        "from threading import Lock\n",
        "from collections import defaultdict\n",
        "\n",
        "myLock = threading.Lock()\n",
        "\n",
        "service = \"runpod\" #\"google_colab\" or \"runpod\"\n",
        "\n",
        "if service == \"runpod\":\n",
        "    script_path = \"/root/ludmila/ludmila\"\n",
        "elif service == \"google_colab\":\n",
        "    script_path = \"/content/drive/My Drive/ludmila/ludmila\"\n",
        "else:\n",
        "    print(\"Unknown service\")\n",
        "    exit(1)\n",
        "\n",
        "\n",
        "dataset_id = 2\n",
        "dataset_filename = \"data\" + str(dataset_id) + \".txt\"\n",
        "\n",
        "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
        "print(\"Device:\", device)\n",
        "\n",
        "REPEAT = 256   #1024â€“8192\n",
        "start, end = -10, 10\n",
        "dtype = torch.int32\n",
        "\n",
        "BATCH_CACHE = None\n",
        "\n",
        "def _rebuild_batch_cache():\n",
        "    global BATCH_CACHE\n",
        "    x_stack, y_stack, const_table = build_batched_inputs(REPEAT)\n",
        "    BATCH_CACHE = {\n",
        "        \"repeat\": REPEAT,\n",
        "        \"x\": x_stack,\n",
        "        \"y\": y_stack,\n",
        "        \"const\": const_table,\n",
        "        \"ncols\": x_stack.shape[1],\n",
        "    }\n",
        "\n",
        "def get_batched_inputs():\n",
        "    global BATCH_CACHE\n",
        "    need = (\n",
        "        BATCH_CACHE is None\n",
        "        or BATCH_CACHE[\"repeat\"] != REPEAT\n",
        "        or BATCH_CACHE[\"ncols\"] != x_vals.numel() * REPEAT\n",
        "    )\n",
        "    if need:\n",
        "        _rebuild_batch_cache()\n",
        "    return BATCH_CACHE[\"x\"], BATCH_CACHE[\"y\"], BATCH_CACHE[\"const\"]\n",
        "\n",
        "\n",
        "def fmt_time(t):\n",
        "    ms = int((t % 1) * 1000)\n",
        "    sec = int(t) % 60\n",
        "    mins = int(t // 60)\n",
        "    return f\"{mins} min {sec} sec {ms} ms\"\n",
        "\n",
        "def op_add(a, b):\n",
        "    return a + b, torch.ones_like(a, dtype=torch.bool)\n",
        "\n",
        "def op_sub(a, b):\n",
        "    return a - b, torch.ones_like(a, dtype=torch.bool)\n",
        "\n",
        "def op_mul(a, b):\n",
        "    return a * b, torch.ones_like(a, dtype=torch.bool)\n",
        "\n",
        "def op_div(a, b):\n",
        "    valid = (b != 0) & (a.remainder(b) == 0)\n",
        "    out = torch.empty_like(a)\n",
        "    out[valid] = torch.div(a[valid], b[valid], rounding_mode='trunc')\n",
        "    out[~valid] = 0\n",
        "    return out, valid\n",
        "\n",
        "def op_pow2(a, _b_ignored):\n",
        "    # (a)^2, always valid\n",
        "    return a * a, torch.ones_like(a, dtype=torch.bool)\n",
        "\n",
        "def op_sqrt(a, _b_ignored):\n",
        "    valid = (a >= 0)\n",
        "    r = torch.sqrt(a.float()).to(a.dtype)\n",
        "    valid = valid & ((r * r) == a)\n",
        "\n",
        "    out = torch.empty_like(a)\n",
        "    out[valid] = r[valid]\n",
        "    out[~valid] = 0\n",
        "    return out, valid\n",
        "\n",
        "\n",
        "OPS = {\n",
        "    '+': op_add,\n",
        "    '-': op_sub,\n",
        "    '*': op_mul,\n",
        "    '/': op_div,\n",
        "    '^2': op_pow2,\n",
        "    '^0.5': op_sqrt,\n",
        "}\n",
        "\n",
        "OPS_FIRST_PASS = {'*', '/', '^2'}\n",
        "\n",
        "CONST_CACHE = {}\n",
        "\n",
        "def operand_tensor(token, x_vals):\n",
        "    if token == 'x':\n",
        "        return x_vals\n",
        "    t = CONST_CACHE.get(token)\n",
        "    if t is None or t.shape != x_vals.shape or t.device != x_vals.device or t.dtype != x_vals.dtype:\n",
        "        t = torch.full_like(x_vals, int(token))\n",
        "        CONST_CACHE[token] = t\n",
        "    return t\n",
        "\n",
        "\n",
        "def eval_expr_tokens(tokens, ops, x_vals):\n",
        "    has_x = any(tok == 'x' for tok in tokens)\n",
        "    if has_x:\n",
        "        base = x_vals\n",
        "        valid = torch.ones_like(x_vals, dtype=torch.bool)\n",
        "    else:\n",
        "        base = torch.tensor([0], device=x_vals.device, dtype=x_vals.dtype)\n",
        "        valid = torch.ones(1, dtype=torch.bool, device=x_vals.device)\n",
        "\n",
        "    vals = [operand_tensor(tok, base) for tok in tokens]\n",
        "\n",
        "    # --- First pass: *, /, ^2 ---\n",
        "    vals2 = [vals[0]]\n",
        "    ops2 = []\n",
        "    trailing_sqrt = False\n",
        "    n_ops = len(ops)\n",
        "\n",
        "    for i, sym in enumerate(ops):\n",
        "        b = vals[i + 1]\n",
        "        is_last = (i == n_ops - 1)\n",
        "        if sym in OPS_FIRST_PASS:\n",
        "            fn = OPS[sym]\n",
        "            cur, v_step = fn(vals2[-1], b)\n",
        "            vals2[-1] = cur\n",
        "            valid = valid & v_step\n",
        "        elif sym == '^0.5' and is_last:\n",
        "            # postpone sqrt until the very end (after + and -)\n",
        "            trailing_sqrt = True\n",
        "            # IMPORTANT: do not add b, it is \"dummy\" for unary operation\n",
        "        else:\n",
        "            # + or - (and any other operator if you want to extend)\n",
        "            ops2.append(sym)\n",
        "            vals2.append(b)\n",
        "\n",
        "    # --- Second pass: + and - ---\n",
        "    res = vals2[0]\n",
        "    for i, sym in enumerate(ops2):\n",
        "        fn = OPS[sym]\n",
        "        b = vals2[i + 1]\n",
        "        res, v_step = fn(res, b)\n",
        "        valid = valid & v_step\n",
        "\n",
        "    # --- Deferred sqrt at the very end ---\n",
        "    if trailing_sqrt:\n",
        "        res, v_step = OPS['^0.5'](res, res)   # b is ignored\n",
        "        valid = valid & v_step\n",
        "\n",
        "    if not has_x:\n",
        "        res = res.expand_as(x_vals)\n",
        "        valid = valid.expand_as(x_vals)\n",
        "\n",
        "    return res, valid, has_x\n",
        "\n",
        "def build_formula(tokens, x_value=None):\n",
        "    parts = []\n",
        "    for i, tok in enumerate(tokens):\n",
        "        val = str(x_value) if (tok == 'x' and x_value is not None) else str(tok)\n",
        "        parts.append(val)\n",
        "        if i < len(tokens) - 1:\n",
        "            parts.append(None)  # placeholder for operator\n",
        "    return parts\n",
        "\n",
        "def stringify(parts, ops):\n",
        "    out = []\n",
        "    op_iter = iter(ops)\n",
        "    for p in parts:\n",
        "        if p is None:\n",
        "            try:\n",
        "                out.append(next(op_iter))\n",
        "            except StopIteration:\n",
        "                # Just in case, but normally won't get here\n",
        "                pass\n",
        "        else:\n",
        "            out.append(str(p))\n",
        "    # If there are leftover operators (e.g., final unary ^0.5) â€” append them at the end\n",
        "    for sym in op_iter:\n",
        "        out.append(sym)\n",
        "    return \" \".join(out)\n",
        "\n",
        "\n",
        "def writeln(str):\n",
        "    myLock.acquire()\n",
        "    with open(script_path + \"/log.txt\", 'a', encoding='utf-8') as the_file:\n",
        "        the_file.write(str + \"\\n\")\n",
        "    myLock.release()\n",
        "\n",
        "def load_dataset(path):\n",
        "    \"\"\"\n",
        "    Expects lines of the format:\n",
        "    y [c1 c2 c3 ...]\n",
        "    Returns a list: [(y, [c1, c2, c3, ...]), ...]\n",
        "    The number of constants after y can be arbitrary (including 0).\n",
        "    Empty lines are ignored.\n",
        "    \"\"\"\n",
        "    ds = []\n",
        "    with open(path, 'r', encoding='utf-8') as f:\n",
        "        for line in f:\n",
        "            line = line.strip()\n",
        "            if not line:\n",
        "                continue\n",
        "            parts = line.split()\n",
        "            # at minimum â€” one number (y)\n",
        "            try:\n",
        "                yv = int(parts[0])\n",
        "            except ValueError:\n",
        "                # line is invalid â€” skip it\n",
        "                continue\n",
        "            consts = []\n",
        "            for p in parts[1:]:\n",
        "                try:\n",
        "                    consts.append(int(p))\n",
        "                except ValueError:\n",
        "                    # invalid token â€” skip only this one\n",
        "                    continue\n",
        "            ds.append((yv, consts))\n",
        "    return ds\n",
        "\n",
        "\n",
        "# --- Mapping constants by indices (key to universal check) ---\n",
        "def make_const_index_map(const_pool):\n",
        "    \"\"\"\n",
        "    Returns dict: {\"51\": 0, \"62\": 1, \"73\": 2} for the current pool.\n",
        "    \"\"\"\n",
        "    return {str(v): i for i, v in enumerate(const_pool)}\n",
        "\n",
        "def remap_tokens_to_target_consts(tokens, base_const_pool, target_consts):\n",
        "    \"\"\"\n",
        "    Transfers the structure of tokens, replacing constants from the base pool\n",
        "    with the corresponding constants of target_consts by INDEX.\n",
        "    Example: tokens = ['x','*','51','+','73'] with target=[52,63,74]\n",
        "    -> ['x','*','52','+','74']\n",
        "    \"\"\"\n",
        "    idx_map = make_const_index_map(base_const_pool)\n",
        "    out = []\n",
        "    for tok in tokens:\n",
        "        if tok == 'x':\n",
        "            out.append('x')\n",
        "        else:\n",
        "            # this is a constant from the base pool\n",
        "            k = idx_map.get(tok, None)\n",
        "            if k is None:\n",
        "                # Protection: if tokens unexpectedly contain a number not from the pool\n",
        "                out.append(tok)\n",
        "            else:\n",
        "                out.append(str(target_consts[k]))\n",
        "    return out\n",
        "\n",
        "def build_batched_inputs(REPEAT=1):\n",
        "    S = len(dataset)\n",
        "    N = x_vals.numel()\n",
        "\n",
        "    # [S, N]\n",
        "    x_stack = x_vals.unsqueeze(0).expand(S, N)\n",
        "    if REPEAT > 1:\n",
        "        x_stack = x_stack.repeat(1, REPEAT)  # [S, N*REPEAT]\n",
        "\n",
        "    y_vec = torch.tensor([y for (y, _) in dataset], device=device, dtype=dtype)\n",
        "    y_stack = y_vec.unsqueeze(1).expand(S, x_stack.shape[1])\n",
        "\n",
        "    const_table = {}\n",
        "    for idx, base_val in enumerate(CONST_POOL_BASE):\n",
        "        per_set = torch.tensor([consts[idx] for (_, consts) in dataset],\n",
        "                               device=device, dtype=dtype)                 # [S]\n",
        "        const_table[str(base_val)] = per_set.unsqueeze(1).expand_as(x_stack) # [S, N*REPEAT]\n",
        "    return x_stack, y_stack, const_table\n",
        "\n",
        "\n",
        "def eval_expr_tokens_batched(tokens, ops, x_stack, const_table):\n",
        "    has_x = any(t == 'x' for t in tokens)\n",
        "    valid = torch.ones_like(x_stack, dtype=torch.bool)\n",
        "\n",
        "    vals = [(x_stack if t == 'x' else const_table[t]) for t in tokens]\n",
        "\n",
        "    # --- pass 1: *, /, ^2 ---\n",
        "    vals2 = [vals[0]]\n",
        "    ops2 = []\n",
        "    trailing_sqrt = False\n",
        "    n_ops = len(ops)\n",
        "\n",
        "    for i, sym in enumerate(ops):\n",
        "        b = vals[i + 1]\n",
        "        is_last = (i == n_ops - 1)\n",
        "        if sym in OPS_FIRST_PASS:\n",
        "            cur, v_step = OPS[sym](vals2[-1], b)\n",
        "            vals2[-1] = cur\n",
        "            valid = valid & v_step\n",
        "        elif sym == '^0.5' and is_last:\n",
        "            trailing_sqrt = True\n",
        "        else:\n",
        "            ops2.append(sym)\n",
        "            vals2.append(b)\n",
        "\n",
        "    # --- pass 2: + Ð¸ - ---\n",
        "    res = vals2[0]\n",
        "    for i, sym in enumerate(ops2):\n",
        "        res, v_step = OPS[sym](res, vals2[i + 1])\n",
        "        valid = valid & v_step\n",
        "\n",
        "    if trailing_sqrt:\n",
        "        res, v_step = OPS['^0.5'](res, res)\n",
        "        valid = valid & v_step\n",
        "\n",
        "    return res, valid, has_x\n",
        "\n",
        "def validate_formula_on_all_sets(tokens_base, ops):\n",
        "    x_stack, y_stack, const_table = get_batched_inputs()\n",
        "\n",
        "    with torch.no_grad():\n",
        "        res, valid, has_x = eval_expr_tokens_batched(tokens_base, ops, x_stack, const_table)\n",
        "        hit = valid & (res == y_stack)                # [S, N*REPEAT]\n",
        "\n",
        "        ok_per_x_expanded = hit.all(dim=0)            # [N*REPEAT]\n",
        "\n",
        "        if REPEAT > 1:\n",
        "            ok_mask = ok_per_x_expanded.view(REPEAT, -1).any(dim=0)  # [N]\n",
        "        else:\n",
        "            ok_mask = ok_per_x_expanded                              # [N]\n",
        "\n",
        "        ok = bool(ok_mask.any().item())\n",
        "\n",
        "        xs_demo = None\n",
        "        if ok:\n",
        "            j = int(torch.nonzero(ok_mask, as_tuple=False).flatten()[0].item())\n",
        "            xs_demo = [int(x_stack[s, j].item()) for s in range(x_stack.shape[0])]\n",
        "\n",
        "    return ok, xs_demo, ok_mask\n",
        "\n",
        "\n",
        "\n",
        "def stringify_pretty(tokens, ops, x_value=None, sqrt_style=\"pow\"):\n",
        "    def tok(t):\n",
        "        return str(x_value) if (t == 'x' and x_value is not None) else str(t)\n",
        "\n",
        "    if not tokens:\n",
        "        return \"\"\n",
        "\n",
        "    out = tok(tokens[0])\n",
        "    i_tok = 1\n",
        "\n",
        "    for i, op in enumerate(ops):\n",
        "        is_last = (i == len(ops) - 1)\n",
        "\n",
        "        if op in ('+','-','*','/'):\n",
        "            if i_tok >= len(tokens): break\n",
        "            out = f\"{out} {op} {tok(tokens[i_tok])}\"\n",
        "            i_tok += 1\n",
        "\n",
        "        elif op == '^2':\n",
        "            out = f\"{out} ^2\"\n",
        "            if i_tok < len(tokens):\n",
        "                i_tok += 1\n",
        "\n",
        "        elif op == '^0.5':\n",
        "            if sqrt_style == \"func\":\n",
        "                out = f\"sqrt({out})\"\n",
        "            else:\n",
        "                out = f\"({out}) ^0.5\"\n",
        "            if i_tok < len(tokens):\n",
        "                i_tok += 1\n",
        "\n",
        "    return out\n",
        "\n",
        "\n",
        "if device == 'cuda':\n",
        "    torch.cuda.synchronize()\n",
        "t0 = time.perf_counter()\n",
        "\n",
        "# Load the whole dataset\n",
        "dataset_path = script_path + \"/datasets/\" + dataset_filename\n",
        "dataset = load_dataset(dataset_path)\n",
        "if not dataset:\n",
        "    raise RuntimeError(f\"Dataset is empty or not read: {dataset_path}\")\n",
        "\n",
        "# The first set is the base for initial search\n",
        "# CHANGED: no checks or comparisons â€” just take the first line from the file\n",
        "y_base, CONST_POOL_BASE = dataset[0]  # CHANGED\n",
        "\n",
        "x_vals = torch.arange(start, end + 1, dtype=dtype, device=device)\n",
        "\n",
        "OPERAND_POOL_BASE = ['x'] + [str(c) for c in CONST_POOL_BASE]\n",
        "\n",
        "# stats:\n",
        "checked_exprs = 0\n",
        "solutions_found_global = 0\n",
        "attempted_eqs_total_base = 0\n",
        "attempted_eqs_by_len_base = defaultdict(int)\n",
        "\n",
        "time_total_start = time.time()\n",
        "\n",
        "OPS_ALPHABET = ['+', '-', '*', '/', '^2', '^0.5']\n",
        "\n",
        "try:\n",
        "    # INFINITE LOOP OF LENGTHS: 1,2,3,4,5, ...\n",
        "    for length in itertools.count(1):  # CHANGED: instead of range(1, 6)\n",
        "        # all operand sequences of length `length`\n",
        "        for tokens in itertools.product(OPERAND_POOL_BASE, repeat=length):\n",
        "\n",
        "            # all operator sequences of the corresponding length\n",
        "            if length == 1:\n",
        "                ops_list = [()]\n",
        "            else:\n",
        "                ops_list = itertools.product(OPS_ALPHABET, repeat=length - 1)\n",
        "\n",
        "            for ops in ops_list:\n",
        "                # Compute on the BASE set (as before)\n",
        "                res, valid, has_x = eval_expr_tokens(tokens, ops, x_vals)\n",
        "                hits = torch.nonzero(valid & (res == y_base), as_tuple=False).flatten()\n",
        "\n",
        "                n_valid_base = int(valid.sum().item())\n",
        "\n",
        "                # stats:\n",
        "                checked_exprs += 1\n",
        "                attempted_eqs_total_base += n_valid_base\n",
        "                attempted_eqs_by_len_base[length] += n_valid_base  # lengths grow without limits\n",
        "\n",
        "                if hits.numel() == 0:\n",
        "                    continue\n",
        "\n",
        "                ok, xs_demo, ok_mask = validate_formula_on_all_sets(list(tokens), ops)\n",
        "                if not ok:\n",
        "                    continue\n",
        "\n",
        "                # Universal formula found â€” log ONCE\n",
        "                common_hits = torch.nonzero(ok_mask, as_tuple=False).flatten()\n",
        "                x_found_base = int(x_vals[common_hits[0]].item()) if common_hits.numel() > 0 else None\n",
        "                formula_str = stringify_pretty(list(tokens), list(ops), x_value=x_found_base, sqrt_style=\"pow\")\n",
        "                time_total = time.time() - time_total_start\n",
        "                message = time.strftime(\"%d.%m.%Y %H:%M:%S\") + \" Solution data\" + str(dataset_id) + \": \" + formula_str + \" at \" + str(round(time_total, 2)) + \" seconds\"\n",
        "\n",
        "                print(message)\n",
        "                writeln(message)\n",
        "                solutions_found_global += 1\n",
        "\n",
        "except KeyboardInterrupt:\n",
        "    # Graceful termination by Ctrl+C with statistics output\n",
        "    if device == 'cuda':\n",
        "        torch.cuda.synchronize()\n",
        "    elapsed = time.perf_counter() - t0\n",
        "    print(\"\\nStopped by user (Ctrl+C). Current results:\")\n",
        "    if solutions_found_global == 0:\n",
        "        print(\"No universal formulas found yet.\")\n",
        "    else:\n",
        "        print(f\"Universal formulas found: {solutions_found_global}\")\n",
        "    print(f\"Base constant set: {CONST_POOL_BASE}\")\n",
        "    print(f\"Range X: [{start}, {end}]\")\n",
        "    print(f\"Checked combinations on base set: ~{checked_exprs:,}\")\n",
        "    print(f\"Time: {fmt_time(elapsed)}\")\n"
      ],
      "metadata": {
        "id": "mKivqcjF2VDb",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "00f065dc-af58-4980-b334-5ac428e687b0"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Device: cuda\n",
            "20.09.2025 18:59:47 Solution data2: (3 ^2 + 4 ^2) ^0.5 at 102.29 seconds\n",
            "20.09.2025 18:59:48 Solution data2: (3 ^2 + 4 ^2) ^0.5 at 103.68 seconds\n",
            "20.09.2025 18:59:49 Solution data2: (3 ^2 + 4 ^2) ^0.5 at 104.62 seconds\n",
            "20.09.2025 18:59:50 Solution data2: (3 ^2 + 4 ^2) ^0.5 at 105.65 seconds\n",
            "20.09.2025 18:59:51 Solution data2: (3 ^2 + 4 ^2) ^0.5 at 106.82 seconds\n",
            "20.09.2025 18:59:52 Solution data2: (3 ^2 + 4 ^2) ^0.5 at 107.79 seconds\n",
            "20.09.2025 18:59:53 Solution data2: (3 ^2 + 4 * 4) ^0.5 at 108.57 seconds\n",
            "20.09.2025 18:59:53 Solution data2: (3 ^2 + 4 ^2) ^0.5 at 108.59 seconds\n",
            "20.09.2025 18:59:54 Solution data2: (3 ^2 + 4 * 4) ^0.5 at 109.41 seconds\n",
            "20.09.2025 18:59:54 Solution data2: (3 ^2 + 4 ^2) ^0.5 at 109.42 seconds\n",
            "20.09.2025 18:59:55 Solution data2: (3 ^2 + 4 * 4) ^0.5 at 110.25 seconds\n",
            "20.09.2025 18:59:55 Solution data2: (3 ^2 + 4 ^2) ^0.5 at 110.26 seconds\n",
            "20.09.2025 19:00:11 Solution data2: (3 * 3 + 4 ^2) ^0.5 at 126.82 seconds\n",
            "20.09.2025 19:00:12 Solution data2: (3 ^2 + 4 ^2) ^0.5 at 127.17 seconds\n",
            "20.09.2025 19:00:12 Solution data2: (3 * 3 + 4 ^2) ^0.5 at 127.7 seconds\n",
            "20.09.2025 19:00:12 Solution data2: (3 ^2 + 4 ^2) ^0.5 at 128.02 seconds\n",
            "20.09.2025 19:00:13 Solution data2: (3 * 3 + 4 ^2) ^0.5 at 128.53 seconds\n",
            "20.09.2025 19:00:13 Solution data2: (3 ^2 + 4 ^2) ^0.5 at 128.88 seconds\n",
            "20.09.2025 19:00:14 Solution data2: (3 * 3 + 4 ^2) ^0.5 at 129.42 seconds\n",
            "20.09.2025 19:00:14 Solution data2: (3 ^2 + 4 ^2) ^0.5 at 129.8 seconds\n",
            "20.09.2025 19:00:15 Solution data2: (3 * 3 + 4 ^2) ^0.5 at 130.39 seconds\n",
            "20.09.2025 19:00:15 Solution data2: (3 ^2 + 4 ^2) ^0.5 at 130.82 seconds\n",
            "20.09.2025 19:00:16 Solution data2: (3 * 3 + 4 ^2) ^0.5 at 131.49 seconds\n",
            "20.09.2025 19:00:16 Solution data2: (3 ^2 + 4 ^2) ^0.5 at 131.91 seconds\n",
            "20.09.2025 19:00:17 Solution data2: (3 * 3 + 4 * 4) ^0.5 at 132.63 seconds\n",
            "20.09.2025 19:00:17 Solution data2: (3 * 3 + 4 ^2) ^0.5 at 132.65 seconds\n",
            "20.09.2025 19:00:17 Solution data2: (3 ^2 + 4 * 4) ^0.5 at 133.15 seconds\n",
            "20.09.2025 19:00:18 Solution data2: (3 ^2 + 4 ^2) ^0.5 at 133.17 seconds\n",
            "20.09.2025 19:00:18 Solution data2: (3 * 3 + 4 * 4) ^0.5 at 133.69 seconds\n",
            "20.09.2025 19:00:18 Solution data2: (3 * 3 + 4 ^2) ^0.5 at 133.71 seconds\n",
            "20.09.2025 19:00:18 Solution data2: (3 ^2 + 4 * 4) ^0.5 at 134.03 seconds\n",
            "20.09.2025 19:00:18 Solution data2: (3 ^2 + 4 ^2) ^0.5 at 134.05 seconds\n",
            "20.09.2025 19:00:19 Solution data2: (3 * 3 + 4 * 4) ^0.5 at 134.56 seconds\n",
            "20.09.2025 19:00:19 Solution data2: (3 * 3 + 4 ^2) ^0.5 at 134.57 seconds\n",
            "20.09.2025 19:00:19 Solution data2: (3 ^2 + 4 * 4) ^0.5 at 134.88 seconds\n",
            "20.09.2025 19:00:19 Solution data2: (3 ^2 + 4 ^2) ^0.5 at 134.91 seconds\n",
            "\n",
            "Stopped by user (Ctrl+C). Current results:\n",
            "Universal formulas found: 36\n",
            "Base constant set: [3, 4]\n",
            "Range X: [-10, 10]\n",
            "Checked combinations on base set: ~211,829\n",
            "Time: 2 min 27 sec 719 ms\n"
          ]
        }
      ]
    }
  ]
}